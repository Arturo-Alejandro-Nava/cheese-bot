import streamlit as st
import google.generativeai as genai
import requests
from bs4 import BeautifulSoup
import os
import time
import glob
import re # We need this for the force-search

# --- CONFIGURATION ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
except:
    st.error("No API Key found. Please add GOOGLE_API_KEY to Streamlit Secrets.")
    st.stop()

genai.configure(api_key=API_KEY)
model = genai.GenerativeModel('gemini-2.0-flash')

st.set_page_config(page_title="Hispanic Cheese Makers-Nuestro Queso", page_icon="ðŸ§€")

# --- HEADER ---
col1, col2 = st.columns([1, 4])
with col1:
    possible_names = ["logo.jpg", "logo.png", "logo.jpeg", "logo"]
    found = False
    for p in possible_names:
        if os.path.exists(p):
            st.image(p, width=130); found = True; break
    if not found: st.write("ðŸ§€")

with col2:
    st.title("Hispanic Cheese Makers-Nuestro Queso")
st.markdown("---")

# --- 1. THE YOUTUBE HUNTER (REGEX) ---
def find_video_links(html_content):
    # This hunts for ANY YouTube link (embed, watch, or shortened) in the raw code
    video_links = []
    # Pattern for youtube IDs
    patterns = [
        r'src="(https://www\.youtube\.com/embed/[\w-]+)',
        r'href="(https://www\.youtube\.com/watch\?v=[\w-]+)',
        r'href="(https://youtu\.be/[\w-]+)'
    ]
    
    for p in patterns:
        matches = re.findall(p, str(html_content))
        for m in matches:
            # Clean URL
            clean = m.split('"')[0].split('?')[0]
            # Convert embed to watchable link
            if "embed" in clean:
                vid_id = clean.split("/")[-1]
                clean = f"https://www.youtube.com/watch?v={vid_id}"
            video_links.append(clean)
            
    return list(set(video_links))

# --- 2. LIVE DATA SCRAPER ---
@st.cache_resource(ttl=3600) 
def get_live_website_data():
    urls = [
        "https://hcmakers.com/category-knowledge/", # Best spot for videos
        "https://hcmakers.com/", 
        "https://hcmakers.com/products/", 
        "https://hcmakers.com/capabilities/",
        "https://hcmakers.com/about-us/"
    ]
    
    combined_data = "LIVE WEBSITE CONTENT:\n"
    found_videos = []
    
    headers = {"User-Agent": "Mozilla/5.0"}
    
    for url in urls:
        try:
            r = requests.get(url, headers=headers)
            content = r.content
            
            # A. Find Videos using "Force-Search"
            videos = find_video_links(content)
            for v in videos:
                found_videos.append(f"- VIDEO ON PAGE '{url}': {v}")
            
            # B. Get Text
            soup = BeautifulSoup(content, 'html.parser')
            text = soup.get_text(" ", strip=True)[:4000]
            combined_data += f"\n--- SOURCE: {url} ---\n{text}\n"
            
        except: continue
        
    # BACKUP HARDCODED LINKS (In case site blocks scraper completely)
    # This guarantees the "Spicy Cheese" video always exists
    found_videos.append("- VIDEO: Spicy Cheese Revolution | LINK: https://www.youtube.com/watch?v=FqG_WcrlKjM")
    found_videos.append("- VIDEO: Factory/Plant Tour | LINK: https://www.youtube.com/watch?v=d_kX27z3KCo") # Generic placeholder or actual if found
    
    # Format video library for AI
    video_section = "\n--- OFFICIAL VIDEO LIBRARY (Share these Links!) ---\n"
    video_section += "\n".join(list(set(found_videos)))
            
    return combined_data + video_section

# --- 3. PDF LOADER ---
@st.cache_resource
def load_manual_pdfs():
    pdf_files = glob.glob("*.pdf")
    active_docs = []
    
    for pdf in pdf_files:
        try:
            remote_file = genai.upload_file(path=pdf, display_name=pdf)
            while remote_file.state.name == "PROCESSING": time.sleep(1); remote_file = genai.get_file(remote_file.name)
            if remote_file.state.name == "ACTIVE": active_docs.append(remote_file)
        except: continue
    return active_docs

# --- INITIAL LOAD ---
with st.spinner("Connecting..."):
    live_web_data = get_live_website_data()
    ai_pdf_files = load_manual_pdfs()

# --- CHAT ENGINE ---
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

def get_answer(question):
    
    system_prompt = f"""
    You are the Senior Sales AI for "Hispanic Cheese Makers-Nuestro Queso".
    
    SOURCES:
    1. **OFFICIAL VIDEO LIBRARY:** I have provided a list of YouTube links below.
    2. **PDF DOCUMENTS (Attached):** For Specs.
    3. **LIVE WEBSITE:** For Info.
    
    RULES:
    1. **SHARING LINKS:** If the user asks for a video, LOOK at the 'OFFICIAL VIDEO LIBRARY' section below.
       - You MUST provide the URL directly.
       - Example Answer: "Here is a video about the Spicy Cheese revolution: [Link]"
    
    2. **NO IMAGES:** Do not show images. Text only.
    
    3. **LANGUAGE:** English or Spanish.
    
    LIVE DATA & VIDEO LIBRARY:
    {live_web_data}
    """
    
    payload = [system_prompt] + ai_pdf_files + [question]
    try: return model.generate_content(payload).text
    except: return "Retrieving link..."

# --- UI ---
for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

with st.form(key="chat_form"):
    user_input = st.text_input("Ask question...")
    submit = st.form_submit_button("Send")

if submit and user_input:
    with st.chat_message("user"):
        st.markdown(user_input)
    st.session_state.chat_history.append({"role": "user", "content": user_input})

    with st.chat_message("assistant"):
        with st.spinner("Searching..."):
            response_text = get_answer(user_input)
            st.markdown(response_text)
            
    st.session_state.chat_history.append({"role": "assistant", "content": response_text})